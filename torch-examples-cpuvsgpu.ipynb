{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code compares the performance of matrix multiplication using PyTorch on the GPU and NumPy on the CPU. It also measures the time taken to create a small tensor of zeros. The @ symbol is used for matrix multiplication in PyTorch. The timings help demonstrate the potential speedup gained by utilizing a GPU for certain operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected device: privateuseone:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_directml\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Check if a GPU (cuda) is available, otherwise use Directl ML, or CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch_directml.is_available():\n",
    "    device = torch_directml.device()\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "# Print the selected device (either 'cuda' or 'cpu')\n",
    "print(f\"selected device: {device}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 0.0171458721\n",
      "CPU times: user 0 ns, sys: 2.45 ms, total: 2.45 ms\n",
      "Wall time: 17.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Measure the elapsed time to create a small tensor with zeros\n",
    "start_time = time.time()\n",
    "# Matrix operation: creating a tensor of zeros\n",
    "zeros = torch.zeros(1, 1)\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed Time: {elapsed_time:.10f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2D tensors\n",
    "torch_rand1 = torch.rand(10000, 10000).to(device)\n",
    "torch_rand2 = torch.rand(10000, 10000).to(device)\n",
    "np_rand1 = torch.rand(10000, 10000)\n",
    "np_rand2 = torch.rand(10000, 10000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time with GPU (torch): 0.5710351467\n",
      "CPU times: user 22.3 ms, sys: 1.76 ms, total: 24.1 ms\n",
      "Wall time: 571 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Measure the elapsed time for matrix multiplication with PyTorch (GPU)\n",
    "start_time = time.time()\n",
    "\n",
    "rand = (torch_rand1 @ torch_rand2)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed Time with GPU (torch): {elapsed_time:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time with CPU (numpy): 2.0472037792\n",
      "CPU times: user 35.4 ms, sys: 2 s, total: 2.04 s\n",
      "Wall time: 2.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Measure the elapsed time for matrix multiplication with NumPy (CPU)\n",
    "start_time = time.time()\n",
    "\n",
    "rand = np.multiply(np_rand1, np_rand2)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed Time with CPU (numpy): {elapsed_time:.10f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 4D tensors\n",
    "torch_rand1 = torch.rand(100, 100, 100, 100).to(device)\n",
    "torch_rand2 = torch.rand(100, 100, 100, 100).to(device)\n",
    "np_rand1 = torch.rand(100, 100, 100, 100)\n",
    "np_rand2 = torch.rand(100, 100, 100, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time with GPU (torch): 0.0280668736\n",
      "CPU times: user 0 ns, sys: 18.9 ms, total: 18.9 ms\n",
      "Wall time: 28.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Measure the elapsed time for 4D matrix multiplication with PyTorch (GPU)\n",
    "start_time = time.time()\n",
    "\n",
    "rand = (torch_rand1 @ torch_rand2)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed Time with GPU (torch): {elapsed_time:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time with CPU (numpy): 1.2041850090\n",
      "CPU times: user 44.7 ms, sys: 1.16 s, total: 1.21 s\n",
      "Wall time: 1.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Measure the elapsed time for 4D matrix multiplication with NumPy (CPU)\n",
    "start_time = time.time()\n",
    "\n",
    "rand = np.multiply(np_rand1, np_rand2)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed Time with CPU (numpy): {elapsed_time:.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code uses PyTorch's multinomial function to draw 10 samples from a multinomial distribution defined by the probabilities tensor. The multinomial distribution is often used when there are multiple possible outcomes with different probabilities, and it returns the indices of the sampled outcomes. The samples tensor will contain 10 indices, each corresponding to a sampled outcome based on the given probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# torch.stack, torch.multinomial, torch.trii, torch.triu, input.T / input.transpose, nn.linear, F.softmax\n",
    "\n",
    "# Define probability tensor\n",
    "# The tensor represents the probabilities of two outcomes: 0 with 10% probability and 1 with 90% probability.\n",
    "# 10% or 0.1 => 0, 90% or 0.9 => 1, each probability points to the index of the probability in the tensor\n",
    "probabilities = torch.tensor([0.1, 0.9])\n",
    "\n",
    "# Draw 10 samples from the multinomial distribution\n",
    "# The multinomial distribution is used to sample indices based on the provided probabilities.\n",
    "# 'num_samples' specifies the number of samples to draw, and 'replacement=True' allows repeated sampling.\n",
    "samples = torch.multinomial(probabilities, num_samples=10, replacement=True)\n",
    "print(samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a tensor with values [1, 2, 3, 4, 5]\n",
    "tensor = torch.tensor([1, 2, 3, 4, 5])\n",
    "\n",
    "# Use torch.cat to concatenate the original tensor with another tensor containing the value 5\n",
    "# The 'dim=0' argument specifies that the concatenation should happen along the first dimension (rows).\n",
    "out = torch.cat((tensor, torch.tensor([5])), dim=0)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Create a 5x5 matrix filled with ones\n",
    "matrix_ones = torch.ones(5, 5)\n",
    "\n",
    "# Use torch.tril to obtain a lower triangular matrix from the input matrix\n",
    "# torch.tril sets all elements above the main diagonal to zero, creating a lower triangular matrix.\n",
    "out = torch.tril(matrix_ones)\n",
    "print(out)\n",
    "\n",
    "# torch.triu sets all elements above the main diagonal to zero, creating a lower triangular matrix.\n",
    "out = torch.triu(matrix_ones)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Create a 5x5 matrix filled with zeros\n",
    "zeros_matrix = torch.zeros(5, 5)\n",
    "\n",
    "# Create a lower triangular matrix with ones\n",
    "lower_triangular_ones = torch.tril(torch.ones(5, 5))\n",
    "\n",
    "# Use masked_fill to replace zeros in the zeros_matrix with negative infinity where lower_triangular_ones is zero\n",
    "# masked_fill replaces elements where the mask is True with the specified value (in this case, negative infinity).\n",
    "out = zeros_matrix.masked_fill(lower_triangular_ones == 0, float('-inf'))\n",
    "\n",
    "\n",
    "# Print the resulting matrix\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Print the exponential values of the tensor 'out'\n",
    "print(torch.exp(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.tensor([1, 2, 3])\n",
    "tensor2 = torch.tensor([4, 5, 6])\n",
    "tensor3 = torch.tensor([7, 8, 9])\n",
    "\n",
    "# Stack the tensors along a new dimension\n",
    "stacked_tensor = torch.stack([tensor1, tensor2, tensor3])\n",
    "stacked_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.2044, -4.5679, -2.6282], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "# Import the PyTorch neural network module\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a tensor with values [10., 10., 10.]\n",
    "sample = torch.tensor([10., 10., 10.])\n",
    "\n",
    "# Create a linear layer with input size 3, output size 3, and no bias term (bias=False)\n",
    "linear = nn.Linear(3, 3, bias=False)\n",
    "\n",
    "# Apply the linear transformation to the input tensor 'sample'\n",
    "output = linear(sample)\n",
    "\n",
    "# Print the resulting tensor\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "# Import the PyTorch functional module\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create a tensor [1.0, 2.0, 3.0]\n",
    "tensor1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# Apply the softmax function to the tensor along dimension 0\n",
    "softmax_output = F.softmax(tensor1, dim=0)\n",
    "# The softmax function is applied element-wise along the specified dimension (dim=0 in this case).\n",
    "\n",
    "# Print the resulting tensor after softmax\n",
    "print(softmax_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9634,  2.9532, -0.3221,  0.3723,  0.9235, -0.6229, -0.3323, -0.5248,\n",
      "         -1.6853, -1.0823,  0.7918,  0.4932, -0.0795, -0.9486, -0.1677,  2.0157,\n",
      "          0.9442, -0.6158, -1.9656,  1.1559, -2.2754,  2.5752, -0.8466, -1.1419,\n",
      "          2.5020, -0.3460, -1.1523, -0.6149, -0.3219, -0.8655,  1.7650,  0.6970,\n",
      "          1.0563,  0.5287, -1.5548, -0.8815,  0.9863,  0.3643, -0.3417, -0.6597,\n",
      "         -1.8015, -0.1843,  2.2012, -0.8728, -0.0036,  1.2241,  0.1246,  1.0306,\n",
      "          0.0470,  0.7017,  0.4331,  0.0430, -1.3249, -1.4835,  0.1120,  0.5813,\n",
      "         -1.1441, -0.1967,  0.6728, -0.7857, -0.7246, -0.2704, -0.6241,  1.6416,\n",
      "          0.6607, -0.5634,  1.1822, -1.2495,  0.4394, -1.5847,  0.5602, -0.0985,\n",
      "         -0.6853,  1.5439,  0.2497, -0.0982, -0.4881,  0.4518, -1.1675, -0.8525,\n",
      "          1.4194,  1.7355, -0.6609,  0.6584,  0.1774, -0.4135, -1.7714, -0.9895,\n",
      "          0.5658,  0.4849,  0.2421, -1.4103,  0.1487,  0.8349,  0.0487,  0.3847,\n",
      "         -0.8402,  1.7945, -1.6846, -1.6588],\n",
      "        [-0.5955, -0.5028, -0.3500, -0.0719,  0.4108, -0.5159,  0.1836, -0.0904,\n",
      "          0.7937,  0.9180, -1.3474,  1.1212,  1.6520,  0.8173,  0.0825,  1.1065,\n",
      "          1.5128, -1.2494,  1.2428,  0.9080, -0.4525, -0.5251, -1.6864, -0.8331,\n",
      "         -1.0929,  0.9445,  0.2720,  0.9260,  1.5523, -0.3306, -1.3919, -1.2101,\n",
      "          1.1130, -0.6418,  0.0701,  0.3713,  0.7224, -0.9703,  1.9775, -1.6260,\n",
      "         -0.6333,  0.0121, -0.4685, -0.6592,  0.2156, -0.7395, -1.7850,  0.0844,\n",
      "          1.2184, -1.0273,  2.4537,  0.4400, -0.7432,  0.2038,  1.7175, -2.2997,\n",
      "          1.2310, -1.3275, -0.9012,  0.7521, -0.4858,  1.4782,  0.8125,  1.6083,\n",
      "          0.7380, -1.4435, -0.4549, -0.8729,  0.0975, -1.8169, -1.6549,  1.3329,\n",
      "          1.4249,  0.8937,  1.4922,  1.0530,  0.7506,  0.7943,  1.7937,  0.1858,\n",
      "         -0.9243, -0.1382,  2.4962,  0.8779, -1.2859, -1.9513, -0.4019,  1.4480,\n",
      "          0.8854, -1.1209,  0.0132, -0.8758, -0.1608,  0.8647, -0.1659,  0.9502,\n",
      "          0.7343, -0.5732,  0.0982, -0.9201],\n",
      "        [-0.4337,  0.8990, -0.9673, -1.4206, -2.4569,  2.1558, -0.6758, -1.6615,\n",
      "         -0.6605,  0.8644, -0.6453, -2.5744, -1.4502, -0.0447,  1.2342,  1.2902,\n",
      "          0.5992, -1.5600,  0.8442,  0.1719,  0.1668, -0.4016,  0.1981, -0.1130,\n",
      "          0.8333,  0.0230, -0.3225, -0.8023,  0.3389,  0.6832,  0.9513, -2.1443,\n",
      "         -0.3053,  1.6671,  0.2143, -1.4816,  1.1656,  0.5170,  2.4823, -0.5552,\n",
      "          0.7115, -2.3212,  0.6949,  0.5630, -1.7227, -1.0794,  0.2745,  0.5345,\n",
      "          0.9109, -0.5236, -1.1442,  1.4529, -0.1901, -1.5144, -0.3368, -0.8071,\n",
      "         -0.4158,  1.9350, -0.5955, -0.2247,  1.5523, -1.9092, -0.8081, -0.2034,\n",
      "         -0.4964,  0.1513, -1.2500,  0.7089,  0.6086,  1.2831, -1.0653,  2.0774,\n",
      "          0.8065, -2.6171, -0.2143,  0.3538,  1.5608, -0.5505,  0.7415, -0.8088,\n",
      "          0.9557,  0.1642, -0.6835,  1.1142, -0.1141, -0.8681,  0.1926,  2.0685,\n",
      "         -0.2168,  0.9710, -0.2815, -0.3717, -0.7994, -0.7386, -0.4886,  1.4518,\n",
      "         -1.2176,  1.2223, -2.5550, -1.8670],\n",
      "        [-0.5842,  0.1024, -1.7629, -0.5409,  1.9782, -1.2774, -0.2230,  1.3739,\n",
      "         -0.9809, -1.2918, -0.7080, -0.7374,  0.0794,  0.4822,  0.9646, -0.5071,\n",
      "          0.5888, -0.3847,  0.3685, -1.0894, -0.5578,  0.9469,  0.3483,  0.1201,\n",
      "         -2.1446,  0.0311,  0.8239,  0.1316, -1.3258,  0.1905,  0.7413,  1.5002,\n",
      "         -0.0674,  0.7472, -0.8574, -1.1549, -0.2631,  1.5819,  0.2156, -1.0006,\n",
      "         -0.3520, -0.4809, -1.2917,  0.9043,  0.9989, -0.8351,  1.1646,  0.1977,\n",
      "          0.2794, -1.0914,  0.3336, -0.3216,  0.6675, -0.3268, -1.2936, -1.1316,\n",
      "          0.1787, -0.9753,  0.9395,  0.7226,  0.1655, -0.0270, -0.0699,  1.1031,\n",
      "          0.0404,  0.1457,  1.2563, -1.3924,  1.3706,  0.0586,  0.7323,  0.1313,\n",
      "         -0.1444, -1.8868,  0.3905, -0.5600,  0.1155, -1.5111,  0.5683, -0.1487,\n",
      "          2.1683, -0.3397,  1.2420,  0.1220,  0.4051, -0.2630, -0.3262, -0.3068,\n",
      "         -0.7616,  2.2702, -0.0357,  0.4781, -0.4012, -0.8949, -2.1767,  0.5533,\n",
      "         -1.8340, -0.1555, -1.1948, -0.2712]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Initialize an embedding layer\n",
    "vocab_size = 10000\n",
    "embedding_dim = 100\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Create some input indices\n",
    "input_indices = torch.LongTensor([1, 5, 3, 2])\n",
    "\n",
    "# Apply the embedding layer\n",
    "embedded_output = embedding(input_indices)\n",
    "\n",
    "# The output will be a tensor of shape (4, 100) where 4 is the number of inputs\n",
    "# and 100 is the dimensionality of the embedding vectors\n",
    "print(embedded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 27,  30,  33],\n",
      "        [ 61,  68,  75],\n",
      "        [ 95, 106, 117]])\n",
      "tensor([[ 27,  30,  33],\n",
      "        [ 61,  68,  75],\n",
      "        [ 95, 106, 117]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "b = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "# Multiply matrixes\n",
    "print(a @ b)\n",
    "print(torch.matmul(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.3927, 3.4966, 4.1921],\n",
      "        [1.3555, 3.4582, 3.2706],\n",
      "        [3.7533, 1.7223, 2.0169]])\n"
     ]
    }
   ],
   "source": [
    "# Generate a random integer tensor with values between 1 (inclusive) and 10 (exclusive),\n",
    "# with a shape of (3, 2), and set the data type to int64\n",
    "int_64 = torch.randint(1, 10, (3, 2)).float()\n",
    "# type int64\n",
    "\n",
    "# Generate a random float tensor with values between 0 and 1,\n",
    "# with a shape of (2, 3), and set the data type to float32\n",
    "float_32 = torch.rand(2, 3)\n",
    "# type float32\n",
    "\n",
    "# Perform matrix multiplication between the integer tensor and the float tensor\n",
    "result = torch.matmul(int_64, float_32)\n",
    "\n",
    "# Print the result of the matrix multiplication\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
